{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching table data for course 01005 in semester 2013-2014\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2014-2015\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2015-2016\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2016-2017\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2017-2018\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2018-2019\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2019-2020\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2020-2021\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2021-2022\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2022-2023\n",
      "No table found.\n",
      "Fetching table data for course 01005 in semester 2023-2024\n",
      "No table found.\n",
      "Data fetching complete. Results saved to dtu_courses_table.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of semesters you want to scrape\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2013, 2024)]\n",
    "\n",
    "# List of course numbers you're interested in\n",
    "course_numbers = [\"01005\"]  # Add more course numbers as needed\n",
    "\n",
    "# Base URL structure\n",
    "base_url = \"https://kurser.dtu.dk/course/\"\n",
    "\n",
    "# Function to fetch course table information\n",
    "def fetch_course_table(semester, course_number):\n",
    "    url = f\"{base_url}{semester}/{course_number}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=60)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find the table with the specific style\n",
    "        table = soup.find(\"table\", {\"style\": \"table-layout:fixed\"})\n",
    "        if table:\n",
    "            # Extract table rows and columns\n",
    "            rows = table.find_all(\"tr\")\n",
    "            table_data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all(\"td\")\n",
    "                cols = [col.get_text(strip=True) for col in cols]\n",
    "                table_data.append(cols)\n",
    "            return table_data\n",
    "        else:\n",
    "            return \"No table found.\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Timeout occurred\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {e}\"\n",
    "\n",
    "# Initialize an empty list to hold the data\n",
    "all_courses_data = []\n",
    "\n",
    "# Loop over each semester and course number\n",
    "for semester in semesters:\n",
    "    for course_number in course_numbers:\n",
    "        print(f\"Fetching table data for course {course_number} in semester {semester}\")\n",
    "        table_data = fetch_course_table(semester, course_number)\n",
    "        if isinstance(table_data, list):  # Ensure we got table data back\n",
    "            # Include semester and course number in each row\n",
    "            for row in table_data:\n",
    "                all_courses_data.append([semester, course_number] + row)\n",
    "        else:\n",
    "            print(table_data)  # Print any error messages\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(all_courses_data, columns=[\"Semester\", \"Course Number\", \"Column1\", \"Column2\", \"Column3\"])  # Adjust column names as needed\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"dtu_courses_table.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data fetching complete. Results saved to dtu_courses_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clarajensen/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'kurser.dtu.dk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<title>kurser.dtu.dk</title>\n",
       "</head>\n",
       "<body>\n",
       "<script>\n",
       "        setTimeout(function() {\n",
       "            window.location.reload();\n",
       "        }, 500);\n",
       "    </script>\n",
       "<iframe id=\"loginFrame\" src=\"?forceLogin=true\" style=\"display:none\"></iframe>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://kurser.dtu.dk/course/2021-2022/01005'\n",
    "headers = {'User-Agent' : 'Clara'}\n",
    "response = requests.get(url, headers=headers, timeout=5, verify=False)\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01005 Matematik 1 (2013/2014)\n",
      "English\n",
      "|\n",
      "Log ind\n",
      "Kursussøgning\n",
      "Studieplanlæggeren\n",
      "01005\n",
      "2013/2014\n",
      "01005 Matematik 1\n",
      "Engelsk titel:\n",
      "Advanced Engineering Mathematics 1\n",
      "Sprog:\n",
      "Dansk\n",
      "Point( ECTS )\n",
      "17,5\n",
      "Kursustype:\n",
      "Bachelor\n",
      "Kurset udbydes under åben uddannelse\n",
      "Skemaplacering:\n",
      "Efterår og Forår\n",
      "Kurset udbydes på tre forskellige skemaplaceringer afhængig af\n",
      "bachelorlinje. Skema A: E1A, E2 og F1A, F2 Skema B: E3A, E4 og F3A,\n",
      "F4 Skema C: E5, E3B og F5, F3B\n",
      "Undervisningens placering:\n",
      "Campus Lyngby\n",
      "Undervisningsform:\n",
      "Pr. uge: 2 forelæsninger, 5 timers\n",
      "gruppearbejde/​klasseundervisning og 2 timers skemalagt\n",
      "selvstudium. Derudover projektarbejde i nogle uger.\n",
      "Kursets varighed:\n",
      "13-uger + 13-uger\n",
      "Eksamensplacering:\n",
      "Særlig dag, Særlig dag\n",
      "Evalueringsform:\n",
      "Skriftlig eksamen og\n",
      "bedømmelse af rapport(er)\n",
      "Evalueringen består af fire delelementer med lige stor vægt: 1) 8\n",
      "sæt hjemmeopgaver 2) En skr. prøve i to dele, en time efter 4. uge\n",
      "og to timer i december/januar. 3) En 3-ugers gruppe-projektopgave.\n",
      "4) En 2-timers skr. prøve i maj.\n",
      "Eksamens varighed:\n",
      "3 timer / 2\n",
      "timer\n",
      "Hjælpemidler:\n",
      "Alle hjælpemidler er\n",
      "tilladt\n",
      "Bedømmelsesform:\n",
      "7-trins skala , ekstern\n",
      "censur\n",
      "Pointspærring:\n",
      "01000\n",
      "/\n",
      "01001\n",
      "/\n",
      "01002\n",
      "/\n",
      "01003\n",
      "/\n",
      "01010\n",
      "/\n",
      "01011\n",
      "/\n",
      "01012\n",
      "/\n",
      "01013\n",
      "/\n",
      "01014\n",
      "/\n",
      "01020\n",
      "/\n",
      "01021\n",
      "/\n",
      "01007\n",
      "Overordnede kursusmål:\n",
      "Kursets emner udgør det matematiske grundlag for en lang række\n",
      "tekniske fag og er samtidig basis for videregående studier inden\n",
      "for matematik og anvendt matematik. Et gennemgående tema er\n",
      "linearitet. Målet er at sætte de studerende i stand til at benytte\n",
      "basale matematiske værktøjer, både teoretisk og i\n",
      "anvendelsesorienterede projekter. Begge aspekter understøttes ved\n",
      "brug af moderne edb-programmer.\n",
      "Læringsmål:\n",
      "En studerende, der fuldt ud har opfyldt kursets mål, vil kunne:\n",
      "Benytte den algebraiske og den geometriske repræsentation af de\n",
      "komplekse tal samt den komplekse eksponentialfunktion.\n",
      "Benytte matrixregning og Gausselimination i forbindelse med\n",
      "løsning af lineære ligningssystemer, og\n",
      "Analysere og forklare løsningsmængder i vektorrum ud fra\n",
      "struktursætningen.\n",
      "Kunne udføre simple beregninger med de elementære funktioner,\n",
      "herunder deres inverse.\n",
      "Benytte de forskellige varianter af Taylors formel til\n",
      "approksimationer og grænseværdibestemmelse.\n",
      "Kunne løse simple første og anden ordens differentialligninger\n",
      "og differentialligningssytemer.\n",
      "Beregne ekstrema for funktioner af flere variable, herunder på\n",
      "områder med rand.\n",
      "Kunne parametrisere simple kurver, flader og rumlige områder,\n",
      "samt beregne simple kurve-, flade- og rumintegraler.\n",
      "Kunne anvende Gauss' og Stokes sætninger i simple\n",
      "sammenhænge.\n",
      "Kunne anvende matematisk terminologi og ræsonnement i\n",
      "forbindelse med mundtlig og skriftlig fremstilling.\n",
      "Organisere samarbejdet i en projektgruppe omkring matematiske\n",
      "begreber og metoder i en større anvendelsesmæssig sammenhæng.\n",
      "Benytte symbolske software-værktøjer, for tiden Maple, til\n",
      "løsning og grafisk illustration af matematiske\n",
      "problemer.\n",
      "Kursusindhold:\n",
      "Lineære ligninger og lineære afbildninger. Matrixalgebra.\n",
      "Vektorrum. Egenværdiproblemet. Symmetriske og ortogonale matricer.\n",
      "Komplekse tal. Lineære differentialligninger. Elementære\n",
      "funktioner. Funktioner af én og flere reelle variable:\n",
      "linearisering og partielle afledede,Taylors formel og kvadratiske\n",
      "former, ekstrema og niveaukurver, flade-, rum-, og kurve-integral.\n",
      "Vektorfelter, Gauss' og Stokes' sætning.\n",
      "Anvendelse af MAPLE i de ovennævnte emner. Anvendelser i\n",
      "ingeniørvidenskaberne.\n",
      "Bemærkninger:\n",
      "Kurset er et to-semesterkursus for bachelor-studerende.\n",
      "Skema A for linjerne Bioteknologi, Kemi & Teknologi,\n",
      "Miljøteknologi, Medicin & Teknologi og Teknisk Biomedicin.\n",
      "Skema B for linjerne Byggeteknologi, Bygningsdesign, Geofysik &\n",
      "rumteknologi, Produktion & Konstruktion og Fysik &\n",
      "Nanoteknologi.\n",
      "Skema C for linjerne Elektroteknologi, Kommunikationsteknologi,\n",
      "Matematik & Teknologi og Softwareteknologi.\n",
      "Mulighed for GRØN DYST deltagelse:\n",
      "Kontakt underviseren for information om hvorvidt dette kursus giver\n",
      "den studerende mulighed for at lave eller forberede et projekt som\n",
      "kan deltage i DTUs studenterkonference om bæredygtighed,\n",
      "klimateknologi og miljø (GRØN DYST). Se mere på\n",
      "http://www.groendyst.dtu.dk\n",
      "Kursusansvarlig:\n",
      "Karsten Schmidt\n",
      ", Bygning 321, rum 005, Tlf. (+45) 4525\n",
      "5856 ,\n",
      "ksch@dtu.dk\n",
      "Michael Pedersen\n",
      ", Bygning 303, rum 103, Tlf. (+45) 4525\n",
      "3012 ,\n",
      "micp@dtu.dk\n",
      "Institut:\n",
      "01\n",
      "Institut for Matematik og Computer Science\n",
      "Kursushjemmeside:\n",
      "http://01005.mat.dtu.dk/\n",
      "Tilmelding:\n",
      "I CampusNet\n",
      "Sidst opdateret: 12. februar, 2014\n",
      "Kopier links for linkning til denne kursusbeskrivelse\n",
      "Denne version:\n",
      "Nyeste version:\n",
      "Denne version\n",
      "vil altid linke til denne version af kursusbeskrivelsen.\n",
      "Nyeste version\n",
      "vil linke til den nyeste version af kursusbeskrivelsen.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_all_text_with_selenium(semester, course_number):\n",
    "    driver = webdriver.Chrome()  # Simple instantiation without custom path or options\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Optional: Scroll to the bottom of the page to trigger loading if needed\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Give it a moment to load\n",
    "        \n",
    "        # Get the page source and parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Get all text from the page\n",
    "        all_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return all_text\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def save_text_to_csv(text_data, filename=\"course_info.csv\"):\n",
    "    # Split the text data by lines\n",
    "    lines = text_data.split(\"\\n\")\n",
    "    \n",
    "    # Write data to a CSV file\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write each line as a row in the CSV file\n",
    "        for line in lines:\n",
    "            writer.writerow([line])\n",
    "\n",
    "# Example usage\n",
    "semester = \"2013-2014\"\n",
    "course_number = \"01005\"\n",
    "page_text = fetch_all_text_with_selenium(semester, course_number)\n",
    "print(page_text)\n",
    "\n",
    "# Save the extracted text to a CSV file\n",
    "save_text_to_csv(page_text, filename=\"course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2013-2014 - 01005\n",
      "Fetched data for 2014-2015 - 01005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b7/xr_h9h1s35l0rntn_yj3w_9h0000gn/T/ipykernel_7189/3103088214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Loop through each semester and scrape data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msemester\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msemesters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mpage_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_all_text_with_selenium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourse_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetched data for {semester} - {course_number}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b7/xr_h9h1s35l0rntn_yj3w_9h0000gn/T/ipykernel_7189/3103088214.py\u001b[0m in \u001b[0;36mfetch_all_text_with_selenium\u001b[0;34m(semester, course_number)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Get the page source and parse with BeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Get all text from the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_url_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             return self.request_encode_url(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py\u001b[0m in \u001b[0;36mrequest_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"?\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     def request_encode_body(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def fetch_all_text_with_selenium(semester, course_number):\n",
    "    options = Options()\n",
    "    options.headless = True  # Run Chrome in headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Optional: Scroll to the bottom of the page to trigger loading if needed\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Give it a moment to load\n",
    "        \n",
    "        # Get the page source and parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Get all text from the page\n",
    "        all_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        return all_text\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def save_text_to_csv(semester, course_number, text_data, filename=\"course_info.csv\"):\n",
    "    # Split the text data by lines\n",
    "    lines = text_data.split(\"\\n\")\n",
    "    \n",
    "    # Write data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write semester, course number, and each line of the text as a row in the CSV file\n",
    "        for line in lines:\n",
    "            writer.writerow([semester, course_number, line])\n",
    "\n",
    "# List of semesters from 2013-2014 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2013, 2024)]\n",
    "\n",
    "# Example usage with multiple semesters\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Clear the CSV file if it exists\n",
    "with open(\"course_info.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Semester\", \"Course Number\", \"Text\"])\n",
    "\n",
    "# Loop through each semester and scrape data\n",
    "for semester in semesters:\n",
    "    page_text = fetch_all_text_with_selenium(semester, course_number)\n",
    "    print(f\"Fetched data for {semester} - {course_number}\")\n",
    "    \n",
    "    # Save the extracted text to a CSV file\n",
    "    save_text_to_csv(semester, course_number, page_text, filename=\"course_info.csv\")\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2017-2018 - 01005\n",
      "Fetched data for 2018-2019 - 01005\n",
      "Fetched data for 2019-2020 - 01005\n",
      "Fetched data for 2020-2021 - 01005\n",
      "Fetched data for 2021-2022 - 01005\n",
      "Fetched data for 2022-2023 - 01005\n",
      "Fetched data for 2023-2024 - 01005\n",
      "Data fetching complete. Results saved to course_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_all_text_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Optional: Scroll to the bottom of the page to trigger loading if needed\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Give it a moment to load\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Get all text from the page\n",
    "    all_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    return all_text\n",
    "\n",
    "def save_text_to_csv(semester, course_number, text_data, filename=\"course_info.csv\"):\n",
    "    # Split the text data by lines\n",
    "    lines = text_data.split(\"\\n\")\n",
    "    \n",
    "    # Write data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write semester, course number, and each line of the text as a row in the CSV file\n",
    "        for line in lines:\n",
    "            writer.writerow([semester, course_number, line])\n",
    "\n",
    "# List of semesters from 2013-2014 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Example usage with multiple semesters\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Clear the CSV file if it exists\n",
    "with open(\"course_info.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Semester\", \"Course Number\", \"Text\"])\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome()  # Simple instantiation without custom path or options\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and scrape data\n",
    "    for semester in semesters:\n",
    "        page_text = fetch_all_text_with_selenium(driver, semester, course_number)\n",
    "        print(f\"Fetched data for {semester} - {course_number}\")\n",
    "        \n",
    "        # Save the extracted text to a CSV file\n",
    "        save_text_to_csv(semester, course_number, page_text, filename=\"course_info.csv\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_all_text_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)  # Give it a moment to load\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Get all text from the page\n",
    "    all_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    return all_text\n",
    "\n",
    "def save_text_to_csv(semester, course_number, text_data, filename=\"course_info.csv\"):\n",
    "    # Split the text data by lines\n",
    "    lines = text_data.split(\"\\n\")\n",
    "    \n",
    "    # Write data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write semester, course number, and each line of the text as a row in the CSV file\n",
    "        for line in lines:\n",
    "            writer.writerow([semester, course_number, line])\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Generate course numbers from 01005 to 88717\n",
    "course_numbers = [str(num).zfill(5) for num in range(1005, 88718)]\n",
    "\n",
    "# Clear the CSV file if it exists\n",
    "with open(\"course_info.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Semester\", \"Course Number\", \"Text\"])\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome()  # Simple instantiation without custom path or options\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and scrape data\n",
    "    for semester in semesters:\n",
    "        page_text = fetch_all_text_with_selenium(driver, semester, course_number)\n",
    "        print(f\"Fetched data for {semester} - {course_number}\")\n",
    "        \n",
    "        # Save the extracted text to a CSV file\n",
    "        save_text_to_csv(semester, course_number, page_text, filename=\"course_info.csv\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for 2017-2018 - 01005\n",
      "Fetched data for 2018-2019 - 01005\n",
      "Fetched data for 2019-2020 - 01005\n",
      "Fetched data for 2020-2021 - 01005\n",
      "Fetched data for 2021-2022 - 01005\n",
      "Fetched data for 2022-2023 - 01005\n",
      "Fetched data for 2023-2024 - 01005\n",
      "Data fetching complete. Results saved to course_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_table_data_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)  # Give it a moment to load fully if necessary\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Find the specific table within the div with class \"box information\"\n",
    "    table = soup.find(\"div\", class_=\"box information\").find(\"table\")\n",
    "    \n",
    "    if table:\n",
    "        # Extract table rows and columns\n",
    "        rows = table.find_all(\"tr\")\n",
    "        table_data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            cols = [col.get_text(strip=True) for col in cols]\n",
    "            table_data.append(cols)\n",
    "        return table_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_table_data_to_csv(semester, course_number, table_data, filename=\"course_info.csv\"):\n",
    "    # Write table data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Semester\", \"Course Number\", \"Column1\", \"Column2\", \"Column3\", \"Column4\"])  # Adjust header names as needed\n",
    "        \n",
    "        # Write each row of the table with the semester and course number\n",
    "        for row in table_data:\n",
    "            writer.writerow([semester, course_number] + row)\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Generate course numbers from 01005 to 88717\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome()  # Simple instantiation without custom path or options\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and course number and scrape data\n",
    "    for semester in semesters:\n",
    "        for course_number in course_numbers:\n",
    "            table_data = fetch_table_data_with_selenium(driver, semester, course_number)\n",
    "            if table_data:\n",
    "                print(f\"Fetched data for {semester} - {course_number}\")\n",
    "                # Save the extracted table data to a CSV file\n",
    "                save_table_data_to_csv(semester, course_number, table_data, filename=\"course_info.csv\")\n",
    "            else:\n",
    "                print(f\"No table found for {semester} - {course_number}\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched information for 2017-2018 - 01005\n",
      "Fetched information for 2018-2019 - 01005\n",
      "Fetched information for 2019-2020 - 01005\n",
      "Fetched information for 2020-2021 - 01005\n",
      "Fetched information for 2021-2022 - 01005\n",
      "Fetched information for 2022-2023 - 01005\n",
      "Fetched information for 2023-2024 - 01005\n",
      "Data fetching complete. Results saved to course_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def fetch_information_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)  # Give it a moment to load fully if necessary\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Find the specific div with class \"information\"\n",
    "    information_div = soup.find(\"div\", class_=\"information\")\n",
    "    \n",
    "    if information_div:\n",
    "        # Extract all the text within the div\n",
    "        information_text = information_div.get_text(separator=\"\\n\", strip=True)\n",
    "        return information_text\n",
    "    else:\n",
    "        return \"No information found\"\n",
    "\n",
    "def save_information_to_csv(semester, course_number, information_text, filename=\"course_info.csv\"):\n",
    "    # Write information to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Semester\", \"Course Number\", \"Information\"])\n",
    "        \n",
    "        # Write the information with the semester and course number\n",
    "        writer.writerow([semester, course_number, information_text])\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Generate course numbers from 01005 to 88717\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: Start maximized\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and course number and scrape data\n",
    "    for semester in semesters:\n",
    "        for course_number in course_numbers:\n",
    "            information_text = fetch_information_with_selenium(driver, semester, course_number)\n",
    "            if information_text:\n",
    "                print(f\"Fetched information for {semester} - {course_number}\")\n",
    "                # Save the extracted information to a CSV file\n",
    "                save_information_to_csv(semester, course_number, information_text, filename=\"course_info.csv\")\n",
    "            else:\n",
    "                print(f\"No information found for {semester} - {course_number}\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Kursusinformation for 2017-2018 - 01005\n",
      "Fetched Kursusinformation for 2018-2019 - 01005\n",
      "Fetched Kursusinformation for 2019-2020 - 01005\n",
      "Fetched Kursusinformation for 2020-2021 - 01005\n",
      "Fetched Kursusinformation for 2021-2022 - 01005\n",
      "Fetched Kursusinformation for 2022-2023 - 01005\n",
      "Fetched Kursusinformation for 2023-2024 - 01005\n",
      "Data fetching complete. Results saved to course_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def fetch_kursusinformation_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)  # Give it a moment to load fully if necessary\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Find the specific div with class \"box information\"\n",
    "    kursusinformation_div = soup.find(\"div\", class_=\"box information\")\n",
    "    \n",
    "    if kursusinformation_div:\n",
    "        # Extract all the text within the Kursusinformation div\n",
    "        kursusinformation_text = kursusinformation_div.get_text(separator=\"\\n\", strip=True)\n",
    "        return kursusinformation_text\n",
    "    else:\n",
    "        return \"No Kursusinformation found\"\n",
    "\n",
    "def save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"course_info.csv\"):\n",
    "    # Write Kursusinformation data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Semester\", \"Course Number\", \"Kursusinformation\"])\n",
    "        \n",
    "        # Write the Kursusinformation with the semester and course number\n",
    "        writer.writerow([semester, course_number, kursusinformation_text])\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Generate course numbers from 01005 to 88717\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: Start maximized\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and course number and scrape data\n",
    "    for semester in semesters:\n",
    "        for course_number in course_numbers:\n",
    "            kursusinformation_text = fetch_kursusinformation_with_selenium(driver, semester, course_number)\n",
    "            if kursusinformation_text:\n",
    "                print(f\"Fetched Kursusinformation for {semester} - {course_number}\")\n",
    "                # Save the extracted Kursusinformation to a CSV file\n",
    "                save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"course_info.csv\")\n",
    "            else:\n",
    "                print(f\"No Kursusinformation found for {semester} - {course_number}\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to course_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Kursusinformation for 2017-2018 - 01005\n",
      "Fetched Kursusinformation for 2018-2019 - 01005\n",
      "Fetched Kursusinformation for 2019-2020 - 01005\n",
      "Fetched Kursusinformation for 2020-2021 - 01005\n",
      "Fetched Kursusinformation for 2021-2022 - 01005\n",
      "Fetched Kursusinformation for 2022-2023 - 01005\n",
      "Fetched Kursusinformation for 2023-2024 - 01005\n",
      "Data fetching complete. Results saved to kursusinformation.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def fetch_kursusinformation_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(2)  # Give it a moment to load fully if necessary\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Find the specific \"Kursusinformation\" table or section within the div\n",
    "    kursusinformation_section = soup.select_one(\"div.box.information\")\n",
    "    \n",
    "    if kursusinformation_section:\n",
    "        # Extract all the text within the Kursusinformation section\n",
    "        kursusinformation_text = kursusinformation_section.get_text(separator=\"\\n\", strip=True)\n",
    "        return kursusinformation_text\n",
    "    else:\n",
    "        return \"No Kursusinformation found\"\n",
    "\n",
    "def save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"kursusinformation.csv\"):\n",
    "    # Write Kursusinformation data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Semester\", \"Course Number\", \"Kursusinformation\"])\n",
    "        \n",
    "        # Write the Kursusinformation with the semester and course number\n",
    "        writer.writerow([semester, course_number, kursusinformation_text])\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2017, 2024)]\n",
    "\n",
    "# Generate course numbers from 01005 to 88717\n",
    "course_number = \"01005\"\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: Start maximized\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and course number and scrape data\n",
    "    for semester in semesters:\n",
    "        for course_number in course_numbers:\n",
    "            kursusinformation_text = fetch_kursusinformation_with_selenium(driver, semester, course_number)\n",
    "            if kursusinformation_text:\n",
    "                print(f\"Fetched Kursusinformation for {semester} - {course_number}\")\n",
    "                # Save the extracted Kursusinformation to a CSV file\n",
    "                save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"kursusinformation.csv\")\n",
    "            else:\n",
    "                print(f\"No Kursusinformation found for {semester} - {course_number}\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to kursusinformation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
