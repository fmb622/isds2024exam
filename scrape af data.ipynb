{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import random\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the list of course IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range of years\n",
    "start_year = 2017\n",
    "end_year = 2024\n",
    "\n",
    "# Example list of department codes (replace these with actual department codes)\n",
    "departments = ['1', '10', '11', '12', '13', '22', '23', '24', '25', '26', '27', '28', '29', '30', '33', '34', '36', '38', '41', '42', '46', '47', '56', '62', '63', '83', '88', 'CB', 'KU']\n",
    "course_codes = set()  # Using a set to automatically handle duplicates\n",
    "\n",
    "# Loop over each academic year\n",
    "for year in range(start_year, end_year + 1):\n",
    "    academic_year = f\"{year}-{year+1}\"\n",
    "    \n",
    "    # Loop over each department code\n",
    "    for dept in departments:\n",
    "        url = f\"https://kurser.dtu.dk/archive/{academic_year}/department/{dept}\"\n",
    "        \n",
    "        # Fetch the page content\n",
    "        response = requests.get(url, headers={'name':'Students from UCPH','email':'lqz683@alumni.ku.dk'})\n",
    "        if response.status_code == 200:  # Proceed only if the page exists\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all course codes (assuming they are in <td> tags inside a table row <tr>)\n",
    "            rows = soup.find_all('tr')\n",
    "\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    # Assuming the course code is in the first cell (td)\n",
    "                    course_code = cells[0].get_text(strip=True)\n",
    "                    course_codes.add(course_code)  # Add to the set to avoid duplicates\n",
    "                     \n",
    "   # Pause before the next request\n",
    "        time.sleep(0.5)  # Sleep for 0.5 seconds \n",
    "\n",
    "# Convert the set to a sorted list\n",
    "course_codes_list = sorted(course_codes)\n",
    "\n",
    "# Create a DataFrame for better visualization and export\n",
    "df = pd.DataFrame(course_codes_list, columns=[\"Course Code\"])\n",
    "\n",
    "# Display the first few rows to check\n",
    "print(df.head())\n",
    "\n",
    "# Save to Excel for further analysis\n",
    "#df.to_excel('course_codes.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the course information from the descriptions on kurser.dtu.dk by scraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kursusinformation_with_selenium(driver, semester, course_number):\n",
    "    url = f\"https://kurser.dtu.dk/course/{semester}/{course_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(random.uniform(1.08, 1.44))  # Give it a moment to load fully if necessary\n",
    "    \n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Find the specific \"Kursusinformation\" table or section within the div\n",
    "    kursusinformation_section = soup.select_one(\"div.box.information\")\n",
    "    \n",
    "    if kursusinformation_section:\n",
    "        # Extract all the text within the Kursusinformation section\n",
    "        kursusinformation_text = kursusinformation_section.get_text(separator=\"\\n\", strip=True)\n",
    "        return kursusinformation_text\n",
    "    else:\n",
    "        return None  # Return None if no information is found\n",
    "\n",
    "def save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"kursusinformation.csv\"):\n",
    "    # Write Kursusinformation data to a CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Semester\", \"Course Number\", \"Kursusinformation\"])\n",
    "        \n",
    "        # Write the Kursusinformation with the semester and course number\n",
    "        writer.writerow([semester, course_number, kursusinformation_text])\n",
    "\n",
    "# List of semesters from 2017-2018 to 2023-2024 (we split the range for more time flexability, but the end result is the same)\n",
    "semesters = [f\"{year}-{year+1}\" for year in range(2023, 2024)]\n",
    "\n",
    "# Generate the first 20 course numbers between 01000 to 88718\n",
    "course_numbers = df['Course Code'].tolist()\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: Start maximized\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "\n",
    "# Start the browser session once\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Loop through each semester and the first 20 course numbers and scrape data\n",
    "    for semester in semesters:\n",
    "        for course_number in course_numbers:\n",
    "            kursusinformation_text = fetch_kursusinformation_with_selenium(driver, semester, course_number)\n",
    "            if kursusinformation_text:\n",
    "                print(f\"Fetched Kursusinformation for {semester} - {course_number}\")\n",
    "                # Save the extracted Kursusinformation to a CSV file\n",
    "                save_kursusinformation_to_csv(semester, course_number, kursusinformation_text, filename=\"kursusinformation.csv\")\n",
    "finally:\n",
    "    # Close the browser session when done\n",
    "    driver.quit()\n",
    "\n",
    "print(\"Data fetching complete. Results saved to kursusinformation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data for GPA statistics from karakterer.dtu.dk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years and periods\n",
    "years = range(2017, 2024+1)  # Adjust as needed\n",
    "seasons = ['Summer', 'Winter']\n",
    "\n",
    "# Prepare the CSV file path\n",
    "output_file_path = './combined_course_data2.csv'\n",
    "\n",
    "# Check if the file already exists; if not, create it with headers\n",
    "if not os.path.exists(output_file_path):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        # Writing header for the first time\n",
    "        f.write('Course,Year,Season,Column1,Column2,...\\n')  # Adjust columns as needed\n",
    "\n",
    "# Loop through each course number, year, and season\n",
    "for course in course_codes:  # Assuming you're using the list course_codes from the Excel file\n",
    "    for year in years:\n",
    "        for season in seasons:\n",
    "            # Construct the URL dynamically\n",
    "            period = f'{season}-{year}'\n",
    "            url = f'https://karakterer.dtu.dk/Histogram/1/{course}/{period}'\n",
    "            \n",
    "            try:\n",
    "                # Parse all tables found on the page\n",
    "                dfs = pd.read_html(url)\n",
    "                \n",
    "                # Check if any tables were found\n",
    "                if dfs:\n",
    "                    # Assuming the first table is the one you want\n",
    "                    df = dfs[0]\n",
    "                    \n",
    "                    # Add additional columns to the DataFrame for course, year, and season\n",
    "                    df.insert(0, 'Course', course)\n",
    "                    df.insert(1, 'Year', year)\n",
    "                    df.insert(2, 'Season', season)\n",
    "            \n",
    "                    \n",
    "                    # Append the DataFrame to the CSV file\n",
    "                    df.to_csv(output_file_path, mode='a', header=False, index=False)\n",
    "                    \n",
    "                    print(f\"Data for {course} during {period} added.\")\n",
    "                else:\n",
    "                    print(f\"No data available for {course} during {period}. Skipping...\")\n",
    "            \n",
    "            except ValueError:\n",
    "                # Handle the case where no tables are found on the page\n",
    "                print(f\"No tables found for {course} during {period}. Skipping...\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {course} during {period}: {e}\")\n",
    "\n",
    "            # Add a delay between each request for different year/season combinations\n",
    "            time.sleep(0.5)  # Pause for 0.5 second \n",
    "\n",
    "print(f\"Data collection completed. Data saved to '{output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the GPA data into a dataframe to inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file = './combined_course_data2.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df_gpa = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming df is the DataFrame you're working with\n",
    "# Step 1: Pivot the DataFrame\n",
    "df_pivot = df_gpa.pivot_table(index=['Course', 'Year', 'Season'], columns='Column1', values='Column2', aggfunc='first').reset_index()\n",
    "\n",
    "# Step 2: Display the transformed DataFrame\n",
    "print(df_pivot.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecesary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Andre versioner\" column\n",
    "df_pivot = df_pivot.drop(columns=['Andre versioner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change GPA data back into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pivoted DataFrame to a new CSV file\n",
    "output_csv_path = './pivoted_course_data.csv'\n",
    "df_pivot.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Step 2: Save the pivoted DataFrame to an Excel file\n",
    "output_excel_path = './pivoted_course_data.xlsx'\n",
    "df_pivot.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change course information data into a data frame to inspect and edit and change it back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the original CSV file into a DataFrame with specified headers\n",
    "csv_file_path = './kursusinformation.csv'  # Update the path as necessary\n",
    "df = pd.read_csv(csv_file_path, header=None, names=['Year', 'CourseCode', 'Kursusinformation'])\n",
    "\n",
    "# Define a function to add space between line shifts\n",
    "def add_space_to_line_shifts(text):\n",
    "    # Replace line shifts (newline characters) with a space followed by the newline character\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "# Apply the function to the \"Kursusinformation\" column\n",
    "df['Kursusinformation'] = df['Kursusinformation'].apply(add_space_to_line_shifts)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame to verify the changes\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file or proceed with further processing\n",
    "output_csv_path = './kursusinformation.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_kursusinformation(info):\n",
    "    # Define the keywords for splitting, including \"Institut\"\n",
    "    keywords = [\n",
    "        \"Engelsk titel\", \"Undervisningssprog\", \"Point( ECTS )\", \"Kursustype\",\n",
    "        \"Undervisningens placering\", \"Undervisningsform\", \"Kursets varighed\", \n",
    "        \"Eksamensplacering\", \"Evalueringsform\", \"Eksamens varighed\",  \n",
    "        \"Hjælpemidler\", \"Bedømmelsesform\", \"Pointspærring\", \n",
    "        \"Anbefale faglige forudsætninger\", \"Anbefalede forudsætninger\", \n",
    "        \"Kursusansvarlig\", \"Medansvarlige\", \"Institut\",\n",
    "        \"Ekstern samarbejdsinstitution\", \n",
    "        \"Tilmelding\", \"Mulighed for GRØN DYST deltagelse\", \n",
    "        \"Deltagende institut\", \"Skemaplacering\", \"Deltagerbegrænsning\", \n",
    "        \"Tidligere kursus\", \"Obligatoriske forudsætninger\"\n",
    "    ]\n",
    "\n",
    "    result = {}\n",
    "    \n",
    "    pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
    "    matches = list(re.finditer(pattern, info))\n",
    "    \n",
    "    # Collect non-Institut data\n",
    "    for i, match in enumerate(matches):\n",
    "        start_keyword = match.group().strip()\n",
    "        start_pos = match.end()\n",
    "\n",
    "        if i + 1 < len(matches):\n",
    "            end_pos = matches[i + 1].start()\n",
    "        else:\n",
    "            end_pos = len(info)\n",
    "\n",
    "        value = info[start_pos:end_pos].strip()\n",
    "        result[start_keyword] = value\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_institut(info):\n",
    "    # Extract every \"Institut\" occurrence and the text that follows it\n",
    "    segments = re.split(r'(Institut)', info)\n",
    "    \n",
    "    institut_data = []\n",
    "    for i in range(len(segments) - 1):\n",
    "        if segments[i] == 'Institut':\n",
    "            institut_data.append('Institut' + segments[i + 1].strip())\n",
    "\n",
    "    return ' '.join(institut_data).strip()\n",
    "\n",
    "# Load the DataFrame\n",
    "csv_file_path = './kursusinformation.csv'\n",
    "df = pd.read_csv(csv_file_path, header=None, names=['Year', 'CourseCode', 'Kursusinformation'])\n",
    "\n",
    "# Split data without handling \"Institut\"\n",
    "split_data = df['Kursusinformation'].apply(split_kursusinformation)\n",
    "split_df = pd.json_normalize(split_data)\n",
    "\n",
    "# Handle \"Institut\" separately\n",
    "split_df['Institut'] = df['Kursusinformation'].apply(extract_institut)\n",
    "\n",
    "# Combine the original columns with the new split columns\n",
    "final_df = pd.concat([df[['Year', 'CourseCode']], split_df], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "output_csv_path = './final_split_kursusinformation.csv'\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Save the final DataFrame to an Excel file\n",
    "output_excel_path = './final_split_kursusinformation.xlsx'\n",
    "final_df.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.read_csv('./pivoted_course_data.csv')  \n",
    "info = pd.read_csv('./final_split_kursusinformation.csv')\n",
    "\n",
    "\n",
    "print(grades.head())\n",
    "print(info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Course' to 'CourseCode' in grades\n",
    "grades.rename(columns={'Course': 'Course code'}, inplace=True)\n",
    "\n",
    "# Rename 'Year' to 'Academic Year' in info\n",
    "info.rename(columns={'CourseCode': 'Course code', 'Year': 'Academic Year'}, inplace=True)\n",
    "\n",
    "# Display the datasets after renaming\n",
    "print(grades.head())\n",
    "print(info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 'Academic Year' column in grades based on 'Year' and 'Season'\n",
    "def create_academic_year(row):\n",
    "    if row['Season'].lower() == 'winter':\n",
    "        return f\"{row['Year']}-{row['Year'] + 1}\"\n",
    "    elif row['Season'].lower() == 'summer':\n",
    "        return f\"{row['Year'] - 1}-{row['Year']}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "grades['Academic Year'] = grades.apply(create_academic_year, axis=1)\n",
    "\n",
    "# Display the modified grades dataset to verify the new 'Academic Year' column\n",
    "print(grades[['Year', 'Season', 'Academic Year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets on 'Course code' and 'Academic Year'\n",
    "merged_df = pd.merge(grades, info, on=['Course code', 'Academic Year'], how='inner')\n",
    "\n",
    "# Reorder columns so that 'Course code' and 'Academic Year' are the first two\n",
    "desired_order = ['Course code', 'Academic Year'] + [col for col in merged_df.columns if col not in ['Course code', 'Academic Year']]\n",
    "merged_df = merged_df[desired_order]\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "output_csv_path = './merged_course_data.csv'\n",
    "merged_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Save the merged dataset to an Excel file\n",
    "output_excel_path = './merged_course_data.xlsx'\n",
    "merged_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# Display a message indicating where the files are saved\n",
    "print(f\"Merged data saved to '{output_csv_path}' and '{output_excel_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Clean data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing duplicates\n",
    "rows_before = merged_df.shape[0]\n",
    "\n",
    "# Remove duplicate rows from the entire DataFrame\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Get the number of rows after removing duplicates\n",
    "rows_after = merged_df.shape[0]\n",
    "\n",
    "# Print the number of rows before and after\n",
    "print(f\"Number of rows before removing duplicates: {rows_before}\")\n",
    "print(f\"Number of rows after removing duplicates: {rows_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "columns_to_drop = [\"Kursusansvarlig\", \"Medansvarlige\", \"Tilmelding\", \"Pointspærring\", \"Skemaplacering\", \n",
    "\"Eksamensplacering\", \"Deltagerbegrænsning\", \"Tidligere kursus\", \"Deltagende institut\",\n",
    "\"Undervisningens placering\", \"Undervisningsform\", \"Kursets varighed\", \"Eksamens varighed\"]\n",
    "clean_merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "cleaned_csv_path = './cleaned_merged_course_data.csv'\n",
    "clean_merged_df.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "# Save the final DataFrame to an Excel file\n",
    "cleaned_excel_path = './cleaned_merged_course_data.xlsx'\n",
    "clean_merged_df.to_excel(cleaned_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'Ekstern samarbejdsinstitution' is treated as a string for processing\n",
    "clean_merged_df['Ekstern samarbejdsinstitution'] = clean_merged_df['Ekstern samarbejdsinstitution'].astype(str)\n",
    "\n",
    "# Set 'Ekstern samarbejdsinstitution' to \"Nej\" if blank, NaN, or contains only whitespace, otherwise \"Ja\"\n",
    "clean_merged_df['Ekstern samarbejdsinstitution'] = np.where(\n",
    "    (clean_merged_df['Ekstern samarbejdsinstitution'].str.strip().isin(['', 'nan'])), \n",
    "    'Nej', \n",
    "    'Ja'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'Anbefale faglige forudsætninger', 'Anbefalede forudsætninger', and 'Obligatoriske forudsætninger' into 'forudsætninger'\n",
    "clean_merged_df['forudsætninger'] = (\n",
    "    clean_merged_df['Anbefale faglige forudsætninger'].fillna('') + ' ' +\n",
    "    clean_merged_df['Anbefalede forudsætninger'].fillna('') + ' ' +\n",
    "    clean_merged_df['Obligatoriske forudsætninger'].fillna('')\n",
    ").str.strip()  # .str.strip() removes any leading or trailing whitespace\n",
    "\n",
    "# Set 'forudsætninger' to \"Nej\" if blank, NaN, or contains \"Ingen\"; otherwise \"Ja\"\n",
    "clean_merged_df['forudsætninger'] = np.where(\n",
    "    (clean_merged_df['forudsætninger'].str.strip() == '') |\n",
    "    (clean_merged_df['forudsætninger'].str.contains(\"Ingen\", case=False, na=False)),\n",
    "    'Nej', \n",
    "    'Ja'\n",
    ")\n",
    "\n",
    "# Drop the old columns 'Anbefale faglige forudsætninger', 'Anbefalede forudsætninger', 'Obligatoriske forudsætninger'\n",
    "clean_merged_df.drop(columns=['Anbefale faglige forudsætninger', 'Anbefalede forudsætninger', 'Obligatoriske forudsætninger'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove rows where \"Eksamensgennemsnit\" is NaN\n",
    "clean_merged_df = clean_merged_df.dropna(subset=['Eksamensgennemsnit'])\n",
    "\n",
    "# Step 2: Remove rows where \"Eksamensgennemsnit\" is \"Intet eksamensgennemsnit\"\n",
    "clean_merged_df = clean_merged_df[clean_merged_df['Eksamensgennemsnit'] != 'Intet eksamensgennemsnit']\n",
    "\n",
    "# Remove the text \"(Efter 7-trinsskalaen)\" from the \"Eksamensgennemsnit\" column\n",
    "clean_merged_df['Eksamensgennemsnit'] = clean_merged_df['Eksamensgennemsnit'].str.replace(' \\(Efter 7-trinsskalaen\\)', '', regex=True)\n",
    "\n",
    "print(f\"Number of rows after cleaning for Eksamensgennemsnit: {len(clean_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the allowed values\n",
    "allowed_values = [\"Kandidat\", \"Bachelor\", \"Ph.d.\", \"Diplomingeniør\", \"Deltidsmaster\"]\n",
    "\n",
    "# Function to filter and retain only allowed values in Kursustype\n",
    "def clean_kursustype(text):\n",
    "    # Split the text into words and filter only allowed values\n",
    "    cleaned_words = [word for word in allowed_values if word in text]\n",
    "    \n",
    "    # Join the matched words with a comma and space, or return NaN if no matches\n",
    "    return ', '.join(cleaned_words) if cleaned_words else pd.NA\n",
    "\n",
    "# Apply the function to the Kursustype column\n",
    "clean_merged_df['Kursustype'] = clean_merged_df['Kursustype'].apply(clean_kursustype)\n",
    "\n",
    "# Drop rows where Kursustype is NaN (i.e., no allowed values were found)\n",
    "clean_merged_df = clean_merged_df.dropna(subset=['Kursustype'])\n",
    "\n",
    "print(f\"Number of rows after cleaning for Kursustype: {len(clean_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean Antal bestået by removing text in parentheses\n",
    "def clean_antal_bestaet(text):\n",
    "    # Use regular expressions to keep only the number before any parentheses\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)', '', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to the Antal bestået column\n",
    "clean_merged_df['Antal bestået'] = clean_merged_df['Antal bestået'].apply(clean_antal_bestaet)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(f\"Number of rows after cleaning for Antal bestået: {len(clean_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'Bedømmelsesform' column by replacing entire lines with \"Intern\" or \"Ekstern\"\n",
    "def clean_bedommelsesform(text):\n",
    "    # Simplify text to \"Intern\" if it contains \"intern\"\n",
    "    if 'intern' in text.lower():\n",
    "        return 'Intern'\n",
    "    \n",
    "    # Simplify text to \"Ekstern\" if it contains \"ekstern\"\n",
    "    if 'ekstern' in text.lower():\n",
    "        return 'Ekstern'\n",
    "    \n",
    "    # If neither \"intern\" nor \"ekstern\" is found, return the text unchanged\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the function to the 'Bedømmelsesform' column\n",
    "clean_merged_df['Bedømmelsesform'] = clean_merged_df['Bedømmelsesform'].apply(clean_bedommelsesform)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(f\"Number of rows after cleaning for Bedømmelsesform: {len(clean_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map the existing values to the desired new values\n",
    "def map_gron_dyst(value):\n",
    "    if pd.isna(value):  # Check if the value is missing (NaN)\n",
    "        return \"Ikke oplyst\"\n",
    "    elif \"Dette kursus giver den studerende en mulighed\" in value:\n",
    "        return \"Ja\"\n",
    "    elif \"Kontakt underviseren for information\" in value:\n",
    "        return \"Måske\"\n",
    "    else:\n",
    "        return value  # Return the original value if no match is found\n",
    "\n",
    "# Apply the function to the Mulighed for GRØN DYST deltagelse column\n",
    "clean_merged_df['Mulighed for GRØN DYST deltagelse'] = clean_merged_df['Mulighed for GRØN DYST deltagelse'].apply(map_gron_dyst)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(f\"Number of rows after cleaning for Mulighed for GRØN DYST deltagelse: {len(clean_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the text\n",
    "def process_institut_text(text):\n",
    "    # Check if the line contains 'Københavns Universitet' and replace it entirely\n",
    "    if \"Københavns Universitet\" in text:\n",
    "        return \"88 Andre kurser\"\n",
    "    \n",
    "    # Remove everything before and including an email address directly in front of 'Institut' if it exists\n",
    "    email_patterns = [r'\\S+@[\\w.-]+\\.dk', r'\\S+@[\\w.-]+\\.com', r'\\S+@[\\w.-]+\\.no']\n",
    "    for pattern in email_patterns:\n",
    "        # Use regex to find an email directly in front of the text 'Institut'\n",
    "        match = re.search(rf'{pattern}\\s+Institut', text)\n",
    "        if match:\n",
    "            email_start = match.start()\n",
    "            text = text[email_start + len(match.group()):].strip()  # Keep everything after the email address and 'Institut'\n",
    "            break  # Only process the first email found\n",
    "\n",
    "    # Remove everything before and including 'Simonsen' if it exists\n",
    "    if \"Simonsen\" in text:\n",
    "        text = text.split(\"Simonsen\", 1)[-1].strip()  # Keep everything after 'Simonsen' and strip extra spaces\n",
    "    \n",
    "    # Define the keywords for truncation\n",
    "    keywords = ['Kursushjemmeside', 'Tilmelding', 'Deltagende', 'Ekstern']\n",
    "    \n",
    "    # Find the position of the first occurrence of any keyword\n",
    "    positions = [text.find(keyword) for keyword in keywords if keyword in text]\n",
    "    \n",
    "    # If any keyword is found, truncate the text from the first occurrence of the keyword\n",
    "    if positions:\n",
    "        min_pos = min(positions)\n",
    "        text = text[:min_pos].strip()  # Return the text up to but not including the keyword and strip extra spaces\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Institut' column\n",
    "clean_merged_df['Institut'] = clean_merged_df['Institut'].apply(process_institut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the 'Institut' text\n",
    "def clean_institut(text):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # If the line begins with \"Institut\", remove it\n",
    "        if text.startswith(\"Institut\"):\n",
    "            text = text.replace(\"Institut\", \"\", 1).strip()  # Remove only the first occurrence of \"Institut\"\n",
    "        \n",
    "        # Replace \"Institutfor\" with \"Institut for\"\n",
    "        text = re.sub(r'\\bInstitutfor\\b', 'Institut for', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Institut' column\n",
    "clean_merged_df['Institut'] = clean_merged_df['Institut'].apply(clean_institut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary for departments\n",
    "departments = {\n",
    "    \"25 Institut for Akvatiske Ressourcer\": \"National Institute of Aquatic Resources\",\n",
    "    \"27 Institut for Bioteknologi og Biomedicin\": \"Department of Biotechnology and Biomedicine\",\n",
    "    \"01 Institut for Matematik og Computer Science\": \"Department of Applied Mathematics and Computer Science\",\n",
    "    \"41 Institut for Byggeri og Mekanisk Teknologi\": \"Department of Civil and Mechanical Engineering\",\n",
    "    \"34 Institut for Elektroteknologi og Fotonik\": \"Department of Electrical and Photonics Engineering\",\n",
    "    \"47 Institut for Energikonvertering- og lagring\": \"Department of Energy Conversion and Storage\",\n",
    "    \"10 Institut for Fysik\": \"Department of Physics\",\n",
    "    \"23 Fødevareinstituttet\": \"National Food Institute\",\n",
    "    \"26 Institut for Kemi\": \"Department of Chemistry\",\n",
    "    \"28 Institut for Kemiteknik\": \"Department of Chemical and Biochemical Engineering\",\n",
    "    \"42 Institut for Teknologi, Ledelse og Økonomi\": \"Department of Technology, Management and Economics\",\n",
    "    \"33 Institut for Mikro- og Nanoteknologi\": \"National Centre for Nano Fabrication and Characterization\",\n",
    "    \"30 Institut for Rumforskning og -teknologi\": \"Department of Space Research and Technology\",\n",
    "    \"22 Institut for Sundhedsteknologi\": \"Department of Health Technology\",\n",
    "    \"12 Institut for Miljø- og Ressourceteknologi\": \"Department of Environmental and Resource Engineering\",\n",
    "    \"46 Institut for Vind og Energisystemer\": \"Department of Wind Energy\",\n",
    "    \"24 Veterinærinstituttet\": \"The Veterinary Institute\",\n",
    "    \"36 DTU Bioinformatik\": \"Institute for Bioinformatics\",\n",
    "    \"88 Andre kurser\": \"Other courses\"\n",
    "}\n",
    "\n",
    "# Extract the leading institute number\n",
    "clean_merged_df['Institut_Number'] = clean_merged_df['Institut'].str.extract(r'(\\d+)', expand=False)\n",
    "\n",
    "# Map Institut \"11\" to \"41\"\n",
    "clean_merged_df['Institut_Number'] = clean_merged_df['Institut_Number'].replace({'11': '41'})\n",
    "\n",
    "# Sort the DataFrame by Year to ensure the most recent names come last\n",
    "clean_merged_df = clean_merged_df.sort_values('Year', ascending=True)\n",
    "\n",
    "# Group by 'Institut_Number' and take the last (most recent) name for each group\n",
    "recent_names = clean_merged_df.groupby('Institut_Number')['Institut'].last().reset_index()\n",
    "\n",
    "# Create a mapping of numbers to their most recent names\n",
    "name_mapping = dict(zip(recent_names['Institut_Number'], recent_names['Institut']))\n",
    "\n",
    "# Replace the Institut names in the original DataFrame with the most recent name\n",
    "clean_merged_df['Institut'] = clean_merged_df['Institut_Number'].map(name_mapping)\n",
    "\n",
    "# Now, use the departments dictionary to translate the names into English\n",
    "clean_merged_df['Institut'] = clean_merged_df['Institut'].replace(departments)\n",
    "\n",
    "# Drop the temporary 'Institut_Number' column\n",
    "clean_merged_df.drop(columns=['Institut_Number'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the 'Hjælpemidler' text\n",
    "def clean_hjælpemidler(text):\n",
    "    # If the line is NaN, blank, or exactly \": i\", replace it with 'Tilladt'\n",
    "    if pd.isna(text) or text.strip() == \"\" or text.strip() == \": i\":\n",
    "        return \"Tilladt\"\n",
    "    \n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # If the line contains \"Uden hjælpemidler\", replace it with \"Ikke tilladt\"\n",
    "        if \"Uden hjælpemidler\" in text:\n",
    "            return \"Ikke tilladt\"\n",
    "        \n",
    "        # If the line contains specific phrases, replace it with 'Tilladt'\n",
    "        if (\"Skriftlige hjælpemidler er tilladt\" in text or \n",
    "            \"Alle hjælpemidler er tilladt\" in text or \n",
    "            \"Karakteren for kurset vil være en helhedsvurdering\" in text or\n",
    "            \"egne noter\" in text):\n",
    "            return \"Tilladt\"\n",
    "        \n",
    "        # If the line contains 'ikke tilladt', return it as it is, unless it contains the specific phrases above\n",
    "        if \"ikke tilladt\" in text.lower() and not any(phrase in text for phrase in [\n",
    "            \"Skriftlige hjælpemidler er tilladt\", \n",
    "            \"Alle hjælpemidler er tilladt\", \n",
    "            \"Karakteren for kurset vil være en helhedsvurdering\",\n",
    "            \"egne noter\"]):\n",
    "            return text\n",
    "        \n",
    "        # If the line contains 'tilladt' but not 'ikke tilladt', replace it with 'Tilladt'\n",
    "        if \"tilladt\" in text.lower():\n",
    "            return \"Tilladt\"\n",
    "    \n",
    "    # Otherwise, return the text unchanged (handles non-string types like NaN)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Hjælpemidler' column\n",
    "clean_merged_df['Hjælpemidler'] = clean_merged_df['Hjælpemidler'].apply(clean_hjælpemidler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the 'Evalueringsform' text\n",
    "def clean_evalueringsform(text):\n",
    "    # Ensure the input is a string and convert it to lowercase for case-insensitive comparison\n",
    "    if isinstance(text, str):\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # First priority: Check for \"Portfolio\" related phrases\n",
    "        if (\"afløsningsopgave\" in text_lower or \n",
    "            \"bedømmelse af opgave\" in text_lower or \n",
    "            \"bedømmelse af øvelse\" in text_lower or\n",
    "            \"bedømmelse af rapport\" in text_lower):\n",
    "            return \"Portfolio\"\n",
    "        \n",
    "        # Second priority: Check for \"Skriftlig eksamen\" only\n",
    "        if \"skriftlig eksamen\" in text_lower and \"mundtlig eksamen\" not in text_lower:\n",
    "            return \"Skriftlig eksamen\"\n",
    "        \n",
    "        # Third priority: Check for \"Mundtlig eksamen\" only\n",
    "        if \"mundtlig eksamen\" in text_lower and \"skriftlig eksamen\" not in text_lower:\n",
    "            return \"Mundtlig eksamen\"\n",
    "        \n",
    "        # Fourth priority: Check if both \"Skriftlig eksamen\" and \"Mundtlig eksamen\" are present\n",
    "        if \"skriftlig eksamen\" in text_lower and \"mundtlig eksamen\" in text_lower:\n",
    "            return \"Skriftlig og mundtlig eksamen\"\n",
    "        \n",
    "        # Final priority: If none of the above conditions are met, return \"Anden\"\n",
    "        return \"Anden\"\n",
    "    \n",
    "    # If the text is not a string (NaN or other), return it unchanged\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Evalueringsform' column\n",
    "clean_merged_df['Evalueringsform'] = clean_merged_df['Evalueringsform'].apply(clean_evalueringsform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_csv_path = './cleaned_merged_course_data.csv'\n",
    "clean_merged_df.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "# Save the cleaned DataFrame to a new Excel file\n",
    "cleaned_excel_path = './cleaned_merged_course_data.xlsx'\n",
    "clean_merged_df.to_excel(cleaned_excel_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
